import os
import argparse
import numpy as np
import sys

from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from transformers import TrainingArguments, Trainer, DataCollatorWithPadding, XLMRobertaForSequenceClassification
from qa.datasets import get_tokenizer_by_name, MultipleChoiceDataset

parser = argparse.ArgumentParser()
parser.add_argument("--model_name", default="xlm-r", type=str, help="The name of the model, can be XLM-R")


parser.add_argument("--num_gpus", default=1, type=int, help="The number of gpus used in this process")
parser.add_argument("--batch_size", default=16, type=int, help="Size of the batch during training")
parser.add_argument("--desired_batch_size", default=64, type=int, help="The desired batch size, generated by the gradient accumulation procedure")
parser.add_argument("--learning_rate", default=2e-4, type=float, help="Learning rate during training")
parser.add_argument("--num_train_epochs", default=20, type=float, help="The number of training epochs")
parser.add_argument("--lr_scheduler_type", default="linear", type=str, help="The type of learning rate scheduller")
parser.add_argument("--weight_decay", default=0.01, type=float, help="Weight decay used by the scheduller")
parser.add_argument("--warmup_ratio", default=0.1, type=float, help="Warmup ratio used in the evolution of learning rate")

def compute_the_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    acc = accuracy_score(labels, predictions)
    prec, rec, f1, _ = precision_recall_fscore_support(
        labels, predictions, average="macro", zero_division=0.0
    )
    print(f"Metrics were computed: acc = {acc}, prec = {prec}, rec = {rec}, f1 = {f1}", file=sys.stderr)
    return {
        "eval_precision": prec,
        "eval_recall": rec,
        "eval_f1": f1,
    }

def get_model_by_name(model_name):
    if model_name == "xlm-r":
        model = XLMRobertaForSequenceClassification.from_pretrained("xlm-roberta-base", num_labels=4)
    return model

def get_model_and_ds(args):
    model = get_model_by_name(args.model_name)
    tokenizer = get_tokenizer_by_name(args.model_name)
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    train_dataset = MultipleChoiceDataset("train", args.model_name)
    val_dataset = MultipleChoiceDataset("valid", args.model_name)
    return model, train_dataset, val_dataset, data_collator


def get_training_arguments(args):
    output_dir = os.path.join("outputs", f'{args.model_name}')
    os.makedirs(output_dir, exist_ok=True)

    return TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=args.num_train_epochs,
        eval_strategy="epoch",
        save_strategy="epoch",
        save_total_limit=2,
        logging_steps=10,
        load_best_model_at_end=True,
        report_to=['tensorboard'],
        metric_for_best_model="eval_f1",
        optim="adamw_torch",
        lr_scheduler_type=args.lr_scheduler_type,
        warmup_ratio=args.warmup_ratio,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=4 * args.batch_size,
        gradient_accumulation_steps=args.desired_batch_size // (args.batch_size * args.num_gpus),
        seed=101,
        dataloader_pin_memory=True,
        remove_unused_columns=False,
        resume_from_checkpoint=False,
    )

def main(args):
    model, train_dataset, val_dataset, data_collator = get_model_and_ds(args)
    training_args = get_training_arguments(args)
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator,
        compute_metrics=compute_the_metrics
    )
    trainer.train()

if __name__ == "__main__":
    args = parser.parse_args([] if "__file__" not in globals() else None)
    main(args)


# TODO: check the LLama 3.1 - check the bigger llama

# TODO: try AYA 1.01
# TODO: finetune - LoRA or other adapters 7B model 
